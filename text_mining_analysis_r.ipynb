{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外交青書2025 テキストマイニング分析（R版）\n",
    "\n",
    "## 概要\n",
    "\n",
    "このノートブックでは、外務省が発行する「外交青書2025」第1章「国際情勢認識と日本外交の展望」のPDFファイルに対して、Rを使用したテキストマイニング分析を行います。\n",
    "\n",
    "### 分析内容\n",
    "\n",
    "1. **PDFからのテキスト抽出** - pdftoolsを使用してPDFからテキストデータを抽出\n",
    "2. **形態素解析** - RMeCabを使用して日本語テキストを単語に分解\n",
    "3. **単語頻度分析** - 出現頻度の高い単語を特定\n",
    "4. **共起分析** - 同時に出現する単語ペアを分析\n",
    "5. **ワードクラウド生成** - 分析結果を視覚化\n",
    "\n",
    "### 使用パッケージ\n",
    "\n",
    "| パッケージ | 用途 |\n",
    "|-----------|------|\n",
    "| pdftools | PDFからテキスト抽出 |\n",
    "| RMeCab | 日本語形態素解析 |\n",
    "| dplyr | データ操作 |\n",
    "| ggplot2 | グラフ描画 |\n",
    "| wordcloud2 | ワードクラウド生成 |\n",
    "| igraph | ネットワーク可視化 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 環境設定\n",
    "\n",
    "必要なパッケージをインストール・読み込みます。\n",
    "\n",
    "### 前提条件\n",
    "- MeCabがシステムにインストールされていること\n",
    "- `sudo apt-get install mecab libmecab-dev mecab-ipadic-utf8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# パッケージのインストール（未インストールの場合）\n",
    "packages <- c(\"pdftools\", \"dplyr\", \"tidyr\", \"ggplot2\", \"stringr\", \n",
    "              \"wordcloud2\", \"igraph\", \"widyr\", \"tidytext\")\n",
    "\n",
    "for (pkg in packages) {\n",
    "  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    install.packages(pkg, repos = \"https://cran.r-project.org\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# RMeCabのインストール（特殊な処理が必要）\n",
    "if (!require(\"RMeCab\", quietly = TRUE)) {\n",
    "  install.packages(\"RMeCab\", repos = \"https://rmecab.jp/R\", type = \"source\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# パッケージの読み込み\n",
    "library(pdftools)\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(ggplot2)\n",
    "library(stringr)\n",
    "library(RMeCab)\n",
    "\n",
    "# 日本語フォント設定\n",
    "theme_set(theme_gray(base_family = \"Noto Sans CJK JP\"))\n",
    "\n",
    "cat(\"パッケージの読み込み完了\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 設定とパラメータ\n",
    "\n",
    "分析に使用する各種パラメータを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ===== ファイル設定 =====\n",
    "PDF_PATH <- \"1_2_1.pdf\"\n",
    "OUTPUT_CSV <- \"word_frequency_results_r.csv\"\n",
    "COOCCURRENCE_CSV <- \"cooccurrence_results_r.csv\"\n",
    "\n",
    "# ===== 形態素解析の設定 =====\n",
    "# 抽出対象の品詞\n",
    "TARGET_POS <- c(\"名詞\", \"動詞\", \"形容詞\")\n",
    "\n",
    "# ストップワード（除外する一般的な単語）\n",
    "STOPWORDS <- c(\n",
    "  # 形式名詞\n",
    "  \"こと\", \"もの\", \"ため\", \"よう\",\n",
    "  # 代名詞\n",
    "  \"これ\", \"それ\", \"あれ\", \"ここ\", \"そこ\", \"あそこ\",\n",
    "  \"どこ\", \"どれ\", \"なに\", \"何\",\n",
    "  # 補助動詞・基本動詞\n",
    "  \"する\", \"いる\", \"ある\", \"なる\", \"れる\", \"られる\", \"せる\",\n",
    "  \"できる\", \"おる\", \"くる\", \"来る\", \"行く\", \"いく\",\n",
    "  # 指示詞\n",
    "  \"この\", \"その\", \"あの\", \"どの\",\n",
    "  # 否定\n",
    "  \"ない\", \"なく\",\n",
    "  # 接尾辞\n",
    "  \"等\", \"的\", \"化\", \"性\", \"上\", \"中\", \"下\", \"内\", \"外\",\n",
    "  # 時間・番号\n",
    "  \"年\", \"月\", \"日\", \"号\", \"第\", \"章\",\n",
    "  # 接続詞等\n",
    "  \"ほか\", \"また\", \"および\", \"かつ\", \"ただし\", \"なお\", \"または\"\n",
    ")\n",
    "\n",
    "cat(\"対象PDF:\", PDF_PATH, \"\\n\")\n",
    "cat(\"抽出品詞:\", paste(TARGET_POS, collapse = \", \"), \"\\n\")\n",
    "cat(\"ストップワード数:\", length(STOPWORDS), \"語\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. PDFからテキスト抽出\n",
    "\n",
    "pdftoolsを使用してPDFファイルからテキストを抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# PDFからテキストを抽出\n",
    "cat(\"PDFからテキストを抽出中...\\n\\n\")\n",
    "\n",
    "pdf_text <- pdf_text(PDF_PATH)\n",
    "cat(\"総ページ数:\", length(pdf_text), \"\\n\")\n",
    "\n",
    "for (i in seq_along(pdf_text)) {\n",
    "  cat(sprintf(\"  ページ %d: %d 文字抽出\\n\", i, nchar(pdf_text[i])))\n",
    "}\n",
    "\n",
    "# 全ページを結合\n",
    "raw_text <- paste(pdf_text, collapse = \"\\n\")\n",
    "cat(sprintf(\"\\n合計抽出文字数: %s 文字\\n\", format(nchar(raw_text), big.mark = \",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 抽出したテキストの先頭部分を確認\n",
    "cat(strrep(\"=\", 60), \"\\n\")\n",
    "cat(\"抽出テキスト（先頭500文字）\\n\")\n",
    "cat(strrep(\"=\", 60), \"\\n\")\n",
    "cat(substr(raw_text, 1, 500), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. テキストの前処理（クリーニング）\n",
    "\n",
    "形態素解析の精度を高めるため、テキストから不要な文字を除去します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "clean_text <- function(text) {\n",
    "  # 数字を除去（半角・全角）\n",
    "  text <- str_replace_all(text, \"[0-9０-９]+\", \"\")\n",
    "  \n",
    "  # 英字を除去（半角・全角）\n",
    "  text <- str_replace_all(text, \"[a-zA-Zａ-ｚＡ-Ｚ]+\", \"\")\n",
    "  \n",
    "  # 記号・空白を除去\n",
    "  text <- str_replace_all(text, \"[（）()【】「」『』・、。：；！？]+\", \" \")\n",
    "  \n",
    "  # 連続する空白を1つに\n",
    "  text <- str_replace_all(text, \"\\\\s+\", \" \")\n",
    "  \n",
    "  return(str_trim(text))\n",
    "}\n",
    "\n",
    "# クリーニングの実行\n",
    "cleaned_text <- clean_text(raw_text)\n",
    "\n",
    "cat(sprintf(\"クリーニング前: %s 文字\\n\", format(nchar(raw_text), big.mark = \",\")))\n",
    "cat(sprintf(\"クリーニング後: %s 文字\\n\", format(nchar(cleaned_text), big.mark = \",\")))\n",
    "cat(sprintf(\"削減率: %.1f%%\\n\", (1 - nchar(cleaned_text) / nchar(raw_text)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 形態素解析\n",
    "\n",
    "RMeCabを使用して日本語テキストを形態素（単語）に分解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 形態素解析を実行\n",
    "cat(\"形態素解析を実行中...\\n\")\n",
    "\n",
    "# テキストを一時ファイルに保存してRMeCabFreqで解析\n",
    "temp_file <- tempfile(fileext = \".txt\")\n",
    "writeLines(cleaned_text, temp_file, useBytes = TRUE)\n",
    "\n",
    "# 形態素解析（品詞情報付き）\n",
    "mecab_result <- RMeCabFreq(temp_file)\n",
    "\n",
    "# 結果をデータフレームに変換\n",
    "words_df <- as.data.frame(mecab_result) %>%\n",
    "  filter(Info1 %in% TARGET_POS) %>%           # 指定品詞のみ\n",
    "  filter(!(Info2 %in% c(\"非自立\", \"代名詞\", \"数\"))) %>%  # サブカテゴリ除外\n",
    "  filter(nchar(Term) > 1) %>%                  # 1文字の単語を除外\n",
    "  filter(!(Term %in% STOPWORDS)) %>%          # ストップワードを除外\n",
    "  arrange(desc(Freq))\n",
    "\n",
    "# 一時ファイルを削除\n",
    "unlink(temp_file)\n",
    "\n",
    "cat(sprintf(\"\\n抽出単語数: %s 語\\n\", format(sum(words_df$Freq), big.mark = \",\")))\n",
    "cat(sprintf(\"ユニーク単語数: %s 語\\n\", format(nrow(words_df), big.mark = \",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 形態素解析の結果サンプルを表示\n",
    "cat(\"抽出された単語（上位20語）:\\n\")\n",
    "head(words_df, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 単語頻度分析\n",
    "\n",
    "抽出した単語の出現回数を分析し、可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 上位30語を表示\n",
    "TOP_N <- 30\n",
    "\n",
    "cat(strrep(\"=\", 60), \"\\n\")\n",
    "cat(sprintf(\"単語頻度ランキング（上位%d語）\\n\", TOP_N))\n",
    "cat(strrep(\"=\", 60), \"\\n\")\n",
    "\n",
    "top_words <- head(words_df, TOP_N)\n",
    "max_freq <- max(top_words$Freq)\n",
    "\n",
    "for (i in 1:nrow(top_words)) {\n",
    "  word <- top_words$Term[i]\n",
    "  freq <- top_words$Freq[i]\n",
    "  bar_len <- round(freq / max_freq * 30)\n",
    "  bar <- paste(rep(\"█\", bar_len), collapse = \"\")\n",
    "  cat(sprintf(\"%2d. %-12s %3d %s\\n\", i, word, freq, bar))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 棒グラフで可視化\n",
    "top20 <- head(words_df, 20)\n",
    "\n",
    "ggplot(top20, aes(x = reorder(Term, Freq), y = Freq)) +\n",
    "  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n",
    "  coord_flip() +\n",
    "  labs(title = \"単語頻度ランキング（上位20語）\",\n",
    "       x = \"単語\",\n",
    "       y = \"出現回数\") +\n",
    "  theme_minimal(base_family = \"Noto Sans CJK JP\") +\n",
    "  theme(axis.text.y = element_text(size = 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. 共起分析\n",
    "\n",
    "同時に出現する単語ペアを分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 文単位で共起分析\n",
    "cat(\"共起分析を実行中...\\n\")\n",
    "\n",
    "# テキストを文に分割\n",
    "sentences <- unlist(str_split(raw_text, \"[。！？\\n]+\"))\n",
    "sentences <- sentences[nchar(sentences) > 10]  # 短すぎる文を除外\n",
    "\n",
    "cat(sprintf(\"文の数: %d\\n\", length(sentences)))\n",
    "\n",
    "# 上位50語のみを共起分析対象とする\n",
    "top50_words <- head(words_df$Term, 50)\n",
    "\n",
    "# 各文から単語を抽出して共起をカウント\n",
    "cooccurrence_list <- list()\n",
    "\n",
    "for (sent in sentences) {\n",
    "  # 文を形態素解析\n",
    "  temp_sent <- tempfile(fileext = \".txt\")\n",
    "  writeLines(sent, temp_sent, useBytes = TRUE)\n",
    "  \n",
    "  tryCatch({\n",
    "    sent_result <- RMeCabFreq(temp_sent)\n",
    "    sent_df <- as.data.frame(sent_result)\n",
    "    \n",
    "    # 上位50語に含まれる単語のみ抽出\n",
    "    sent_words <- unique(sent_df$Term[sent_df$Term %in% top50_words])\n",
    "    \n",
    "    # 単語ペアを作成\n",
    "    if (length(sent_words) >= 2) {\n",
    "      pairs <- combn(sort(sent_words), 2, simplify = FALSE)\n",
    "      for (pair in pairs) {\n",
    "        key <- paste(pair, collapse = \"-\")\n",
    "        if (is.null(cooccurrence_list[[key]])) {\n",
    "          cooccurrence_list[[key]] <- 0\n",
    "        }\n",
    "        cooccurrence_list[[key]] <- cooccurrence_list[[key]] + 1\n",
    "      }\n",
    "    }\n",
    "  }, error = function(e) {})\n",
    "  \n",
    "  unlink(temp_sent)\n",
    "}\n",
    "\n",
    "# 共起データフレームを作成\n",
    "cooccurrence_df <- data.frame(\n",
    "  pair = names(cooccurrence_list),\n",
    "  count = unlist(cooccurrence_list),\n",
    "  stringsAsFactors = FALSE\n",
    ") %>%\n",
    "  arrange(desc(count)) %>%\n",
    "  separate(pair, into = c(\"word1\", \"word2\"), sep = \"-\")\n",
    "\n",
    "cat(sprintf(\"\\n共起ペア数: %s ペア\\n\", format(nrow(cooccurrence_df), big.mark = \",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 共起頻度上位20ペアを表示\n",
    "cat(strrep(\"=\", 50), \"\\n\")\n",
    "cat(\"共起頻度ランキング（上位20ペア）\\n\")\n",
    "cat(strrep(\"=\", 50), \"\\n\")\n",
    "\n",
    "top20_pairs <- head(cooccurrence_df, 20)\n",
    "for (i in 1:nrow(top20_pairs)) {\n",
    "  cat(sprintf(\"%2d. %s - %s: %d回\\n\", \n",
    "              i, \n",
    "              top20_pairs$word1[i], \n",
    "              top20_pairs$word2[i], \n",
    "              top20_pairs$count[i]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 共起ネットワークの可視化\n",
    "library(igraph)\n",
    "\n",
    "# 上位40ペアでネットワーク作成\n",
    "top_pairs <- head(cooccurrence_df, 40) %>%\n",
    "  filter(count >= 2)\n",
    "\n",
    "if (nrow(top_pairs) > 0) {\n",
    "  # グラフを作成\n",
    "  g <- graph_from_data_frame(top_pairs[, c(\"word1\", \"word2\", \"count\")], directed = FALSE)\n",
    "  E(g)$weight <- top_pairs$count\n",
    "  \n",
    "  # ノードサイズを次数に基づいて設定\n",
    "  V(g)$size <- degree(g) * 3 + 10\n",
    "  \n",
    "  # エッジの太さを重みに基づいて設定\n",
    "  E(g)$width <- E(g)$weight * 0.5\n",
    "  \n",
    "  # プロット\n",
    "  par(mar = c(0, 0, 2, 0))\n",
    "  plot(g,\n",
    "       layout = layout_with_fr(g),\n",
    "       vertex.color = \"lightblue\",\n",
    "       vertex.frame.color = \"darkblue\",\n",
    "       vertex.label.family = \"Noto Sans CJK JP\",\n",
    "       vertex.label.cex = 0.9,\n",
    "       vertex.label.color = \"black\",\n",
    "       edge.color = \"gray70\",\n",
    "       main = \"共起ネットワーク\")\n",
    "  \n",
    "  cat(sprintf(\"\\nノード数: %d\\n\", vcount(g)))\n",
    "  cat(sprintf(\"エッジ数: %d\\n\", ecount(g)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. 品詞別分析\n",
    "\n",
    "テキスト全体の品詞構成を分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 品詞別の出現回数\n",
    "temp_file <- tempfile(fileext = \".txt\")\n",
    "writeLines(cleaned_text, temp_file, useBytes = TRUE)\n",
    "all_mecab <- RMeCabFreq(temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "pos_df <- as.data.frame(all_mecab) %>%\n",
    "  group_by(Info1) %>%\n",
    "  summarise(count = sum(Freq), .groups = \"drop\") %>%\n",
    "  arrange(desc(count)) %>%\n",
    "  mutate(percentage = count / sum(count) * 100)\n",
    "\n",
    "cat(strrep(\"=\", 40), \"\\n\")\n",
    "cat(\"品詞別出現回数\\n\")\n",
    "cat(strrep(\"=\", 40), \"\\n\")\n",
    "\n",
    "head(pos_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 円グラフで可視化\n",
    "top_pos <- head(pos_df, 6)\n",
    "\n",
    "ggplot(top_pos, aes(x = \"\", y = count, fill = Info1)) +\n",
    "  geom_bar(stat = \"identity\", width = 1) +\n",
    "  coord_polar(\"y\", start = 0) +\n",
    "  labs(title = \"品詞構成\", fill = \"品詞\") +\n",
    "  theme_minimal(base_family = \"Noto Sans CJK JP\") +\n",
    "  theme(axis.text = element_blank(),\n",
    "        axis.title = element_blank(),\n",
    "        panel.grid = element_blank()) +\n",
    "  geom_text(aes(label = sprintf(\"%.1f%%\", percentage)),\n",
    "            position = position_stack(vjust = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ワードクラウド生成\n",
    "\n",
    "単語の出現頻度を視覚的に表現するワードクラウドを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# wordcloud2を使用したワードクラウド\n",
    "library(wordcloud2)\n",
    "\n",
    "# データを準備\n",
    "wc_data <- words_df %>%\n",
    "  select(word = Term, freq = Freq) %>%\n",
    "  head(100)\n",
    "\n",
    "# ワードクラウドを生成\n",
    "wordcloud2(wc_data, \n",
    "           size = 0.8,\n",
    "           color = \"random-dark\",\n",
    "           backgroundColor = \"white\",\n",
    "           fontFamily = \"Noto Sans CJK JP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. 結果の保存\n",
    "\n",
    "分析結果をCSVファイルに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 単語頻度結果を保存\n",
    "output_words <- words_df %>%\n",
    "  mutate(rank = row_number()) %>%\n",
    "  select(順位 = rank, 単語 = Term, 品詞 = Info1, 出現回数 = Freq)\n",
    "\n",
    "write.csv(output_words, OUTPUT_CSV, row.names = FALSE, fileEncoding = \"UTF-8\")\n",
    "cat(sprintf(\"単語頻度結果を保存しました: %s\\n\", OUTPUT_CSV))\n",
    "cat(sprintf(\"保存単語数: %d 語\\n\", nrow(output_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 共起分析結果を保存\n",
    "output_cooc <- cooccurrence_df %>%\n",
    "  mutate(rank = row_number()) %>%\n",
    "  select(順位 = rank, 単語1 = word1, 単語2 = word2, 共起回数 = count)\n",
    "\n",
    "write.csv(output_cooc, COOCCURRENCE_CSV, row.names = FALSE, fileEncoding = \"UTF-8\")\n",
    "cat(sprintf(\"共起分析結果を保存しました: %s\\n\", COOCCURRENCE_CSV))\n",
    "cat(sprintf(\"保存ペア数: %d ペア\\n\", nrow(output_cooc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. 分析結果のまとめ\n",
    "\n",
    "外交青書2025 第1章の分析結果をまとめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 分析結果のサマリー\n",
    "cat(strrep(\"=\", 60), \"\\n\")\n",
    "cat(\"分析結果サマリー（R版）\\n\")\n",
    "cat(strrep(\"=\", 60), \"\\n\")\n",
    "\n",
    "cat(\"\\n【基本統計】\\n\")\n",
    "cat(sprintf(\"  抽出文字数: %s 文字\\n\", format(nchar(raw_text), big.mark = \",\")))\n",
    "cat(sprintf(\"  総単語数: %s 語\\n\", format(sum(words_df$Freq), big.mark = \",\")))\n",
    "cat(sprintf(\"  ユニーク単語数: %s 語\\n\", format(nrow(words_df), big.mark = \",\")))\n",
    "cat(sprintf(\"  共起ペア数: %s ペア\\n\", format(nrow(cooccurrence_df), big.mark = \",\")))\n",
    "\n",
    "cat(\"\\n【上位10語】\\n\")\n",
    "for (i in 1:min(10, nrow(words_df))) {\n",
    "  cat(sprintf(\"  %2d. %s: %d回\\n\", i, words_df$Term[i], words_df$Freq[i]))\n",
    "}\n",
    "\n",
    "cat(\"\\n【共起頻度 上位5ペア】\\n\")\n",
    "for (i in 1:min(5, nrow(cooccurrence_df))) {\n",
    "  cat(sprintf(\"  %d. %s - %s: %d回\\n\", \n",
    "              i, \n",
    "              cooccurrence_df$word1[i], \n",
    "              cooccurrence_df$word2[i], \n",
    "              cooccurrence_df$count[i]))\n",
    "}\n",
    "\n",
    "cat(\"\\n【出力ファイル】\\n\")\n",
    "cat(sprintf(\"  - %s: 単語頻度データ\\n\", OUTPUT_CSV))\n",
    "cat(sprintf(\"  - %s: 共起分析データ\\n\", COOCCURRENCE_CSV))\n",
    "\n",
    "cat(\"\\n\", strrep(\"=\", 60), \"\\n\")\n",
    "cat(\"分析完了\\n\")\n",
    "cat(strrep(\"=\", 60), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 付録: セッション情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
